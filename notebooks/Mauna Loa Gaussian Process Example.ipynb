{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is based on the example in section 5.4.3, in selecting a kernel function that makes sense for the Mauna Loa $CO_2$ data. Similar concepts can be applied to our dataset of bike data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ExpSineSquared, RationalQuadratic, WhiteKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "`sklearn` provides us with the dummy dataset used for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_mldata('mauna-loa-atmospheric-co2').data\n",
    "X = data[:, [1]]\n",
    "y = data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance Function Definition\n",
    "\n",
    "We follow closely the covariance functions defined in RW, and we learn them with our Maximum Likelihood Estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long term smooth rising trend\n",
    "\n",
    "We make use the a squared exponential covariance term with **two** hyperparameters controlling amplitude $\\theta_1$ and characteristic length-scale $\\theta_2$.\n",
    "\n",
    "$$k_1(x, x') = \\theta_1^2 \\text{exp}\\Big( - \\frac{(x - x')^2}{2 \\theta_2^2} \\Big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k1(theta1, theta2):\n",
    "    return theta1**2 * RBF(length_scale=theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decay from exact periodicity\n",
    "\n",
    "The seasonal trend of $CO_2$ concentration is not exactly periodic. We can take the product with another RBF to allow a *decay away* from the exact periodicity.\n",
    "\n",
    "$$k_2(x, x') = \\theta_3^2 \\text{exp} \\Big(- \\frac{(x - x')^2}{2 \\theta_4^2} - \\frac{2 \\text{sin}^2 (\\pi (x - x'))}{\\theta_5^2} \\Big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k2(theta3, theta4, theta5):\n",
    "    return theta3**2 * RBF(length_scale=theta4) \\\n",
    "        * ExpSineSquared(length_scale=theta5, periodicity=1.0,\n",
    "                         periodicity_bounds=\"fixed\") # seasonal component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium term irregularities\n",
    "\n",
    "(I am not very sure what is this)\n",
    "\n",
    "A rational quadratic term is used here.\n",
    "\n",
    "$$k_3(x, x') = \\theta_6^2 \\Big( 1 + \\frac{(x - x')^2}{2\\theta_8 \\theta_7^2} \\Big)^{- \\theta_8 }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k3(theta6, theta7, theta8):\n",
    "    return theta6**2 * RationalQuadratic(length_scale=theta7, alpha=theta8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Model\n",
    "\n",
    "We model the noise as the sum of a squared exponential contribution and an independent component.\n",
    "\n",
    "$$k4(x_p, x_q) = \\theta_9^2 \\text{exp} \\Big( - \\frac{(x_p - x_q)^2}{2 \\theta_10^2} \\Big) + \\theta_{11}^2\\delta_{pq}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k4(theta9, theta10, theta11):\n",
    "    return 0.1**2 * RBF(length_scale=0.1) \\\n",
    "        + WhiteKernel(noise_level=0.1**2,\n",
    "                      noise_level_bounds=(1e-3, np.inf)) # noise terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Our final covariance function is simply the sum of all covariance functions.\n",
    "\n",
    "$$k(x, x') = k_1(x, x') + k_2(x, x') + k_3(x, x') + k_4(x, x'),$$\n",
    "\n",
    "with $[\\theta_1 \\ldots \\theta_{11}]$ hyperparameters to optimize.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_all(thetas):\n",
    "    return k1(thetas[0], thetas[1]) + k2(thetas[2], thetas[3], thetas[4])\\\n",
    "        + k3(thetas[5], thetas[6], thetas[7]) + k4(thetas[8], thetas[9], thetas[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We are going to make use of `sklearn`'s Gaussian Process model to fit our data with the defined covariance function. Let's first try out random $\\theta$ and see how it goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods to know what's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_likelihood(gp):\n",
    "    print(\"\\nLearned kernel: %s\" % gp.kernel_)\n",
    "    print(\"Log-marginal-likelihood: %.3f\"\n",
    "          % gp.log_marginal_likelihood(gp.kernel_.theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize random hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.random.rand(11).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Parameters\n",
    "\n",
    "Just for the fun of it, let's see how we perform with random hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcessRegressor(kernel=k_all(thetas), alpha=0, normalize_y=True, optimizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcessRegressor(alpha=0, copy_X_train=True,\n",
       "             kernel=0.868**2 * RBF(length_scale=0.525) + 0.934**2 * RBF(length_scale=0.369) * ExpSineSquared(length_scale=0.215, periodicity=1) + 0.855**2 * RationalQuadratic(alpha=0.258, length_scale=0.681) + 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01),\n",
       "             n_restarts_optimizer=0, normalize_y=True, optimizer=None,\n",
       "             random_state=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned kernel: 0.868**2 * RBF(length_scale=0.525) + 0.934**2 * RBF(length_scale=0.369) * ExpSineSquared(length_scale=0.215, periodicity=1) + 0.855**2 * RationalQuadratic(alpha=0.258, length_scale=0.681) + 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01)\n",
      "Log-marginal-likelihood: -2455.752\n"
     ]
    }
   ],
   "source": [
    "print_likelihood(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah that sucks, turns out that `sklearn`'s GPR has a neat little feature that bakes in hyperparameter optimization within the fitting process. Let's try that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Parameters\n",
    "\n",
    "Turns out that the `optimizer` kwarg defaults to `fmin_l_bfgs_b`. Which I have no idea what that is. Need to read a bit more about this. But either way, it is tuning the hyperparameters to maximize our log marginal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcessRegressor(kernel=k_all(thetas), alpha=0, normalize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcessRegressor(alpha=0, copy_X_train=True,\n",
       "             kernel=0.868**2 * RBF(length_scale=0.525) + 0.934**2 * RBF(length_scale=0.369) * ExpSineSquared(length_scale=0.215, periodicity=1) + 0.855**2 * RationalQuadratic(alpha=0.258, length_scale=0.681) + 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01),\n",
       "             n_restarts_optimizer=0, normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b', random_state=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned kernel: 0.442**2 * RBF(length_scale=0.951) + 3.27**2 * RBF(length_scale=181) * ExpSineSquared(length_scale=1.44, periodicity=1) + 34.4**2 * RationalQuadratic(alpha=1.45e+04, length_scale=41.7) + 0.198**2 * RBF(length_scale=0.138) + WhiteKernel(noise_level=0.0336)\n",
      "Log-marginal-likelihood: -83.221\n"
     ]
    }
   ],
   "source": [
    "print_likelihood(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-83.214` is better than the result that the textbook gave, which is around `-108.5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Conclusion\n",
    "\n",
    "The `GaussianProcessRegressor` provided by `sklearn` is pretty simple to use. The kernel definition is pretty customizable for our usage as well. \n",
    "\n",
    "The convenience brought about by the `optimizer` also cuts down a lot of boilerplate code that we might have to write to run hyperparameter optimization.\n",
    "\n",
    "Here are several parts that we should explore a little bit more:\n",
    "\n",
    "- **Validation**: how do we know that what we've predicted is accurate?\n",
    "- **Visualizations**: plotting of the graph\n",
    "\n",
    "Validation is an important step because we want to ensure that we know our model predicted is reliable for our planning purposes in the second part of the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
